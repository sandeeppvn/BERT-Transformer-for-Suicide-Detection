{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Import train and test split functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import Bert tokenizer and transformer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "import datasets\n",
    "\n",
    "# Import Trainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Import pytorch\n",
    "import torch\n",
    "# Import dataset and dataloader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# Import tensorboard for logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer_path = os.path.join('./tensorboard_logs')\n",
    "writer = SummaryWriter(writer_path)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from source\n",
    "!gdown 1RX3DgPDdVv8wXg43L2tzV6eAJvG7OgkY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>post_body</th>\n",
       "      <th>post_title</th>\n",
       "      <th>cleaned_body</th>\n",
       "      <th>cleaned_title</th>\n",
       "      <th>title_length</th>\n",
       "      <th>post_length</th>\n",
       "      <th>sad</th>\n",
       "      <th>labels</th>\n",
       "      <th>negative_cnt2</th>\n",
       "      <th>suicidal_degree</th>\n",
       "      <th>number_of_posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>I'm stuck and I hate it. I have zero confidenc...</td>\n",
       "      <td>I can't see another way out. Nothing is workin...</td>\n",
       "      <td>stuck hate confidence abilities future stuck d...</td>\n",
       "      <td>working sliding back</td>\n",
       "      <td>7.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.593416</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>1.426405</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>706</td>\n",
       "      <td>_URL_ Please head over to this link.He needs h...</td>\n",
       "      <td>I don't know what else to do...[Please help]</td>\n",
       "      <td>url head link figured professional give hope r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.278830</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>981</td>\n",
       "      <td>I don't know. I've always had depression. Some...</td>\n",
       "      <td>Point of Existence</td>\n",
       "      <td>depression worse worse moved country year ago ...</td>\n",
       "      <td>point existence</td>\n",
       "      <td>3.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>0.463558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>1.420861</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1233</td>\n",
       "      <td>[15 M] I only stay around because I'm afraid m...</td>\n",
       "      <td>I just want to be happy again...</td>\n",
       "      <td>stay afraid friend kill care make killing read...</td>\n",
       "      <td>happy</td>\n",
       "      <td>7.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.210069</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1.224538</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1318</td>\n",
       "      <td>So um, hey everyone.I'm not sure really what t...</td>\n",
       "      <td>Referred here by someone on a forum...</td>\n",
       "      <td>um hey put depression issues long time yay pub...</td>\n",
       "      <td>referred forum</td>\n",
       "      <td>7.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>1.483701</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                          post_body  \\\n",
       "0       16  I'm stuck and I hate it. I have zero confidenc...   \n",
       "1      706  _URL_ Please head over to this link.He needs h...   \n",
       "2      981  I don't know. I've always had depression. Some...   \n",
       "3     1233  [15 M] I only stay around because I'm afraid m...   \n",
       "4     1318  So um, hey everyone.I'm not sure really what t...   \n",
       "\n",
       "                                          post_title  \\\n",
       "0  I can't see another way out. Nothing is workin...   \n",
       "1       I don't know what else to do...[Please help]   \n",
       "2                                 Point of Existence   \n",
       "3                   I just want to be happy again...   \n",
       "4             Referred here by someone on a forum...   \n",
       "\n",
       "                                        cleaned_body          cleaned_title  \\\n",
       "0  stuck hate confidence abilities future stuck d...   working sliding back   \n",
       "1  url head link figured professional give hope r...                    NaN   \n",
       "2  depression worse worse moved country year ago ...        point existence   \n",
       "3  stay afraid friend kill care make killing read...                  happy   \n",
       "4  um hey put depression issues long time yay pub...         referred forum   \n",
       "\n",
       "   title_length  post_length       sad  labels  negative_cnt2  \\\n",
       "0           7.0        100.0  0.593416     1.0       1.945910   \n",
       "1           8.0         56.0  0.000000     0.0       0.000000   \n",
       "2           3.0        325.0  0.463558     0.0       2.890372   \n",
       "3           7.0        111.0  0.210069     1.0       1.791759   \n",
       "4           7.0        185.0  0.390625     1.0       2.197225   \n",
       "\n",
       "   suicidal_degree  number_of_posts  \n",
       "0         1.426405              2.0  \n",
       "1         1.278830              1.0  \n",
       "2         1.420861              1.0  \n",
       "3         1.224538              1.0  \n",
       "4         1.483701              1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('final_feature_data.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nagas/.conda/envs/641/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107.06498194945848 189.84790661501842\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['cleaned_post_length'] = df['cleaned_body'].apply(lambda x: len(x.split(' ')))\n",
    "# Plot the distribution of post length\n",
    "sns.distplot(df['cleaned_post_length'])\n",
    "# Print mean and std of post length\n",
    "print(df['cleaned_post_length'].mean(), df['cleaned_post_length'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     277.000000\n",
       "mean      107.064982\n",
       "std       189.847907\n",
       "min         6.000000\n",
       "25%        30.000000\n",
       "50%        55.000000\n",
       "75%       119.000000\n",
       "max      2534.000000\n",
       "Name: cleaned_post_length, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find mean and std of post length\n",
    "df['cleaned_post_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 165\n",
      "Validation set size: 42\n",
      "Test set size: 52\n"
     ]
    }
   ],
   "source": [
    "# Perform train/valid/test split with stratified sampling\n",
    "X = df.drop(['labels'], axis=1)\n",
    "y = df.labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "\n",
    "# Print the size of the train, valid, and test sets\n",
    "print(f'Train set size: {len(X_train)}')\n",
    "print(f'Validation set size: {len(X_val)}')\n",
    "print(f'Test set size: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show distribution of labels in train, valid, and test sets\n",
    "# Display plots next to each other\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.countplot(x=y_train, data=X_train)\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.countplot(x=y_val, data=X_val)\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.countplot(x=y_test, data=X_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/nagas/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/nagas/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/nagas/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Data Sets\n",
    "\n",
    "# Create Encoders using BertTokenizer\n",
    "train_encodings = tokenizer(\n",
    "    X_train['cleaned_body'].tolist(),\n",
    "    padding=True,\n",
    "    max_length = 90,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False,\n",
    "    # return_tensors='pt',\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    X_val['cleaned_body'].tolist(),\n",
    "    padding=True,\n",
    "    max_length = 90,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False,\n",
    "    # return_tensors='pt',\n",
    ")\n",
    "\n",
    "# Create dataset class\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            k: torch.tensor(v[idx]) for k, v in self.encodings.items()\n",
    "        }\n",
    "        item['labels'] = torch.tensor(int(self.labels.iloc[idx]))\n",
    "\n",
    "\n",
    "        return item\n",
    "\n",
    "train_dataset = BertDataset(train_encodings, y_train)\n",
    "val_dataset = BertDataset(val_encodings, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/nagas/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/nagas/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 109483778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', \n",
    "    num_labels=2, \n",
    "    output_hidden_states=False, \n",
    "    output_attentions=False\n",
    ")\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total number of parameters: {pytorch_total_params}')\n",
    "\n",
    "# Set device if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# Use TrainingArguments and Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./models/bert',\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=20,\n",
    "    num_train_epochs=4,\n",
    "    logging_first_step=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    adam_epsilon=1e-06,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_steps=500,\n",
    ")\n",
    "\n",
    "metric = datasets.load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = preds.argmax(-1)\n",
    "    return metric.compute(references=preds,predictions=labels)\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 165\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/24 04:55 < 04:55, 0.04 it/s, Epoch 2/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.652100</td>\n",
       "      <td>0.694758</td>\n",
       "      <td>{'accuracy': 0.5238095238095238}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.652100</td>\n",
       "      <td>0.693986</td>\n",
       "      <td>{'accuracy': 0.5714285714285714}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 32\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.5238095238095238}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to ./models/bert/checkpoint-6\n",
      "Configuration saved in ./models/bert/checkpoint-6/config.json\n",
      "Model weights saved in ./models/bert/checkpoint-6/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 32\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.5714285714285714}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to ./models/bert/checkpoint-12\n",
      "Configuration saved in ./models/bert/checkpoint-12/config.json\n",
      "Model weights saved in ./models/bert/checkpoint-12/pytorch_model.bin\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/gpfs/data1/cmongp/Sandeep/Github/MSML_641_Project/Train.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgsapp17.umd.edu/gpfs/data1/cmongp/Sandeep/Github/MSML_641_Project/Train.ipynb#ch0000035vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgsapp17.umd.edu/gpfs/data1/cmongp/Sandeep/Github/MSML_641_Project/Train.ipynb#ch0000035vscode-remote?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py:1512\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1508'>1509</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1510'>1511</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1511'>1512</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1513'>1514</a>\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1514'>1515</a>\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1515'>1516</a>\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py:1628\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1624'>1625</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, epoch, metrics)\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1626'>1627</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n\u001b[0;32m-> <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1627'>1628</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_checkpoint(model, trial, metrics\u001b[39m=\u001b[39;49mmetrics)\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1628'>1629</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_save(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py:1748\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1741'>1742</a>\u001b[0m metric_value \u001b[39m=\u001b[39m metrics[metric_to_check]\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1743'>1744</a>\u001b[0m operator \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mgreater \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgreater_is_better \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mless\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1744'>1745</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1745'>1746</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mbest_metric \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1746'>1747</a>\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mbest_model_checkpoint \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1747'>1748</a>\u001b[0m     \u001b[39mor\u001b[39;00m operator(metric_value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate\u001b[39m.\u001b[39;49mbest_metric)\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1748'>1749</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1749'>1750</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mbest_metric \u001b[39m=\u001b[39m metric_value\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.9/site-packages/transformers/trainer.py?line=1750'>1751</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mbest_model_checkpoint \u001b[39m=\u001b[39m output_dir\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, train_dataloader, val_dataloader, optimizer, learning_rate_scheduler, num_epochs, device):\n",
    "    # Set device\n",
    "    model.to(device)\n",
    "\n",
    "    train_accuracies = []\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        \n",
    "        \n",
    "        ######## TRAINING #########        \n",
    "\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Set traingin loss and accuracy to 0\n",
    "        train_accuracy = 0\n",
    "        train_loss = 0\n",
    "\n",
    "        for data in train_dataloader:\n",
    "\n",
    "            # Set the gradients to zero for each batch\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Set the input and labels to the device by getting to right dimension\n",
    "            input_ids= data['input_ids'].squeeze(1).to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            batch_loss, logits = model(\n",
    "                input_ids=input_ids, \n",
    "                token_type_ids=None,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            # Add loss to total loss\n",
    "            train_loss += batch_loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Clip the gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update scheduler\n",
    "            learning_rate_scheduler.step()\n",
    "\n",
    "            # Get the training accuracy\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            train_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "        # Calculate average loss and accuracy\n",
    "        avg_loss = train_loss / len(train_dataloader)\n",
    "        avg_accuracy = train_accuracy / len(train_dataloader)\n",
    "\n",
    "        # Add to tensorboard\n",
    "        writer.add_scalar('train/Loss', avg_loss, epoch)\n",
    "        writer.add_scalar('train/Accuracy', avg_accuracy, epoch)\n",
    "\n",
    "        # Add loss and accuracy to lists\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(avg_accuracy)\n",
    "\n",
    "\n",
    "        ######## VALIDATION #########\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Calculate validation accuracy and loss to 0\n",
    "        val_accuracy = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        for data in train_dataloader:\n",
    "            \n",
    "            # Set the input and labels to the device by getting to right dimension\n",
    "            input_ids= data['input_ids'].squeeze(1).to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Forward pass\n",
    "                batch_loss, logits = model(\n",
    "                    input_ids=input_ids, \n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "            # Add loss to total loss\n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            labels = labels.to('cpu').numpy()\n",
    "\n",
    "            # Calculate accuracy            \n",
    "            val_accuracy += get_accuracy(logits, labels)\n",
    "            \n",
    "        # Calculate average loss and accuracy\n",
    "        avg_loss = val_loss / len(val_dataloader)\n",
    "        avg_val_accuracy = val_accuracy / len(val_dataloader)\n",
    "\n",
    "        # Add to tensorboard\n",
    "        writer.add_scalar('valid/Loss', avg_loss, epoch)\n",
    "        writer.add_scalar('valid/Accuracy', avg_val_accuracy, epoch)\n",
    "\n",
    "        # Add validation accuracy and loss to lists\n",
    "        val_accuracies.append(avg_val_accuracy)\n",
    "        val_losses.append(avg_loss)\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Training Loss: {avg_loss}')\n",
    "    print(f'Training Accuracy: {avg_accuracy}')\n",
    "    print(f'Validation Accuracy: {avg_val_accuracy}')\n",
    "    print('\\n')\n",
    "\n",
    "    # Return the model and the lists\n",
    "    return model, accuracies, losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "classifier, accuracies, losses, val_accuracies = train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    optimizer,\n",
    "    learning_rate_scheduler,\n",
    "    num_epochs,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "torch.save(classifier.state_dict(), './model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the saved model, load the model\n",
    "classifier.load_state_dict(torch.load('./model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results from the loaded model\n",
    "plt.plot(accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the tensorboard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f4b116549ec8be66ddae058514bc5a2ede973036ed9498cb511b27f845d4c8d"
  },
  "kernelspec": {
   "display_name": "Python ('MSML_641')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
