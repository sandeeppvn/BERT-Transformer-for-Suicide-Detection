{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "import torch\n",
    "from utils import TransformerDataset, TransformerModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load X_train and y_train, X_val and y_val from pickle files\n",
    "X_train = pd.read_pickle('X_train.pkl')\n",
    "y_train = pd.read_pickle('y_train.pkl')\n",
    "X_val = pd.read_pickle('X_val.pkl')\n",
    "y_val = pd.read_pickle('y_val.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "train_encodings = tokenizer(\n",
    "    X_train['cleaned_body'].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False,\n",
    "    max_length=256,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "val_encodings = tokenizer(\n",
    "    X_val['cleaned_body'].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False,\n",
    "    max_length=256,\n",
    "    return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=['cleaned_body'], inplace=True)\n",
    "X_val.drop(columns=['cleaned_body'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "batch_size = 16\n",
    "num_labels = 2\n",
    "\n",
    "# Set device if cuda is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TransformerDataset(train_encodings, y_train)\n",
    "val_dataset = TransformerDataset(val_encodings, y_val)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and training parameters\n",
    "num_epochs = 20\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "model = TransformerModel(num_labels)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "learning_rate_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(train_dataloader) * num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "train_losses,val_losses,train_accuracies,val_accuracies = [],[],[],[]\n",
    "train_recalls,val_recalls = [],[]\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "    ######## TRAINING #########\n",
    "    train_accuracy, train_loss, val_accuracy, val_loss = 0, 0, 0, 0\n",
    "    train_recall, val_recall = 0, 0\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, predictions = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        batch_loss = criterion(logits, labels)\n",
    "        train_loss += batch_loss.item()\n",
    "        train_accuracy += accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        train_recall += recall_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "\n",
    "        # Backward pass\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        learning_rate_scheduler.step()\n",
    "\n",
    "    train_losses.append(train_loss/len(train_dataloader))\n",
    "    train_accuracies.append(train_accuracy/len(train_dataloader))\n",
    "    train_recalls.append(train_recall/len(train_dataloader))\n",
    "\n",
    "    ######## VALIDATION #########\n",
    "    model.eval()\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, predictions = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        batch_loss = criterion(logits, labels)\n",
    "        val_loss += batch_loss.item()\n",
    "        val_accuracy += accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        val_recall += recall_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "\n",
    "    val_losses.append(val_loss/len(val_dataloader))\n",
    "    val_accuracies.append(val_accuracy/len(val_dataloader))\n",
    "    val_recalls.append(val_recall/len(val_dataloader))\n",
    "\n",
    "    \n",
    "\n",
    "print(f'Train Loss: {train_loss/len(train_dataloader)}')\n",
    "print(f'Train Accuracy: {train_accuracy/len(train_dataloader)}')\n",
    "print(f'Train Recall: {train_recall/len(train_dataloader)}')\n",
    "\n",
    "print(f'Validation Loss: {val_loss/len(val_dataloader)}')\n",
    "print(f'Validation Accuracy: {val_accuracy/len(val_dataloader)}')\n",
    "print(f'Validation Recall: {val_recall/len(val_dataloader)}')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS: 4\n",
    "\n",
    "# Plot triain and validation metrics\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(train_recalls, label='Train Recall')\n",
    "plt.plot(val_recalls, label='Validation Recall')\n",
    "plt.legend()\n",
    "plt.title('Recall')\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "model=RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./outputs',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy='steps'\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 207\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 70\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/gpfs/data1/cmongp/Sandeep/Github/MSML_641_Project/Train_Transformer.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgsapp6/gpfs/data1/cmongp/Sandeep/Github/MSML_641_Project/Train_Transformer.ipynb#ch0000011vscode-remote?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py:1422\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=1419'>1420</a>\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=1420'>1421</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=1421'>1422</a>\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=1423'>1424</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=1424'>1425</a>\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=1425'>1426</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=1426'>1427</a>\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=1427'>1428</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=1428'>1429</a>\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=1429'>1430</a>\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py:2011\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=2007'>2008</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=2009'>2010</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=2010'>2011</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=2012'>2013</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=2013'>2014</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py:2043\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=2040'>2041</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=2041'>2042</a>\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=2042'>2043</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=2043'>2044</a>\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=2044'>2045</a>\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/transformers/trainer.py?line=2045'>2046</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/641/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:158\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py?line=152'>153</a>\u001b[0m     \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mdevice \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj:\n\u001b[1;32m    <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py?line=153'>154</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule must have its parameters and buffers \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py?line=154'>155</a>\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mon device \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m (device_ids[0]) but found one of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py?line=155'>156</a>\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mthem on device: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj, t\u001b[39m.\u001b[39mdevice))\n\u001b[0;32m--> <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py?line=157'>158</a>\u001b[0m inputs, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscatter(inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids)\n\u001b[1;32m    <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py?line=158'>159</a>\u001b[0m \u001b[39m# for forward function without any inputs, empty list and dict will be created\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py?line=159'>160</a>\u001b[0m \u001b[39m# so the module can be executed on one device which is the first one in device_ids\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py?line=160'>161</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m inputs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:175\u001b[0m, in \u001b[0;36mDataParallel.scatter\u001b[0;34m(self, inputs, kwargs, device_ids)\u001b[0m\n\u001b[1;32m    <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py?line=173'>174</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscatter\u001b[39m(\u001b[39mself\u001b[39m, inputs, kwargs, device_ids):\n\u001b[0;32m--> <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py?line=174'>175</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m scatter_kwargs(inputs, kwargs, device_ids, dim\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdim)\n",
      "File \u001b[0;32m~/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:45\u001b[0m, in \u001b[0;36mscatter_kwargs\u001b[0;34m(inputs, kwargs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=42'>43</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Scatter with support for kwargs dictionary\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=43'>44</a>\u001b[0m inputs \u001b[39m=\u001b[39m scatter(inputs, target_gpus, dim) \u001b[39mif\u001b[39;00m inputs \u001b[39melse\u001b[39;00m []\n\u001b[0;32m---> <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=44'>45</a>\u001b[0m kwargs \u001b[39m=\u001b[39m scatter(kwargs, target_gpus, dim) \u001b[39mif\u001b[39;00m kwargs \u001b[39melse\u001b[39;00m []\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=45'>46</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(kwargs):\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=46'>47</a>\u001b[0m     inputs\u001b[39m.\u001b[39mextend(() \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(kwargs) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(inputs)))\n",
      "File \u001b[0;32m~/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:36\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(inputs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=29'>30</a>\u001b[0m \u001b[39m# After scatter_map is called, a scatter_map cell will exist. This cell\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=30'>31</a>\u001b[0m \u001b[39m# has a reference to the actual function scatter_map, which has references\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=31'>32</a>\u001b[0m \u001b[39m# to a closure that has a reference to the scatter_map cell (because the\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=32'>33</a>\u001b[0m \u001b[39m# fn is recursive). To avoid this reference cycle, we set the function to\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=33'>34</a>\u001b[0m \u001b[39m# None, clearing the cell\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=34'>35</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=35'>36</a>\u001b[0m     res \u001b[39m=\u001b[39m scatter_map(inputs)\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=36'>37</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=37'>38</a>\u001b[0m     scatter_map \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:27\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mlist\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(scatter_map, obj))]\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=25'>26</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(obj) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=26'>27</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mtype\u001b[39m(obj)(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(scatter_map, obj\u001b[39m.\u001b[39;49mitems()))]\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m [obj \u001b[39mfor\u001b[39;00m targets \u001b[39min\u001b[39;00m target_gpus]\n",
      "File \u001b[0;32m~/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:23\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mtype\u001b[39m(obj)(\u001b[39m*\u001b[39margs) \u001b[39mfor\u001b[39;00m args \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(scatter_map, obj))]\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(obj) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(scatter_map, obj)))\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(obj) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mlist\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(scatter_map, obj))]\n",
      "File \u001b[0;32m~/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:19\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscatter_map\u001b[39m(obj):\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=17'>18</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=18'>19</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m Scatter\u001b[39m.\u001b[39;49mapply(target_gpus, \u001b[39mNone\u001b[39;49;00m, dim, obj)\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=19'>20</a>\u001b[0m     \u001b[39mif\u001b[39;00m is_namedtuple(obj):\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py?line=20'>21</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m [\u001b[39mtype\u001b[39m(obj)(\u001b[39m*\u001b[39margs) \u001b[39mfor\u001b[39;00m args \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(scatter_map, obj))]\n",
      "File \u001b[0;32m~/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:96\u001b[0m, in \u001b[0;36mScatter.forward\u001b[0;34m(ctx, target_gpus, chunk_sizes, dim, input)\u001b[0m\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/_functions.py?line=92'>93</a>\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39mand\u001b[39;00m ctx\u001b[39m.\u001b[39minput_device \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/_functions.py?line=93'>94</a>\u001b[0m     \u001b[39m# Perform CPU to GPU copies in a background stream\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/_functions.py?line=94'>95</a>\u001b[0m     streams \u001b[39m=\u001b[39m [_get_stream(device) \u001b[39mfor\u001b[39;00m device \u001b[39min\u001b[39;00m target_gpus]\n\u001b[0;32m---> <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/_functions.py?line=95'>96</a>\u001b[0m outputs \u001b[39m=\u001b[39m comm\u001b[39m.\u001b[39;49mscatter(\u001b[39minput\u001b[39;49m, target_gpus, chunk_sizes, ctx\u001b[39m.\u001b[39;49mdim, streams)\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/_functions.py?line=96'>97</a>\u001b[0m \u001b[39m# Synchronize with the copy stream\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/_functions.py?line=97'>98</a>\u001b[0m \u001b[39mif\u001b[39;00m streams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/comm.py:189\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(tensor, devices, chunk_sizes, dim, streams, out)\u001b[0m\n\u001b[1;32m    <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/comm.py?line=186'>187</a>\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/comm.py?line=187'>188</a>\u001b[0m     devices \u001b[39m=\u001b[39m [_get_device_index(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m devices]\n\u001b[0;32m--> <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/comm.py?line=188'>189</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_scatter(tensor, devices, chunk_sizes, dim, streams))\n\u001b[1;32m    <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/comm.py?line=189'>190</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/nagas/.conda/envs/641/lib/python3.10/site-packages/torch/nn/parallel/comm.py?line=190'>191</a>\u001b[0m     \u001b[39mif\u001b[39;00m devices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de60a245e7dc293b94b7f15a43bb5013c16e37f6d6afa880548d377c1c47b003"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('641')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
